{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment1_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "wxJFY3E1pSPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "import re\n",
        "import io\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import os\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ILd6F6O2pRNY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmQLPxRWq54e",
        "outputId": "b5c4d94f-f185-4caa-a44a-d607d6d63277"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Unzip"
      ],
      "metadata": {
        "id": "wDKMfK5-pMNW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1C_jHeloOdT",
        "outputId": "ae193886-6076-4451-9891-bbf772021fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#('/content/MyDrive/MyDrive/IR/Humor,Hist,Media,Food.zip')\n",
        "!unzip /content/MyDrive/MyDrive/IR/Humor,Hist,Media,Food.zip > /dev/null"
      ],
      "metadata": {
        "id": "DUp17zM0p2bJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path='/content/Humor,Hist,Media,Food'\n",
        "alldirs = [dataset_path+'/'+d for d in os.listdir(os.path.join(dataset_path))]\n",
        "alldirs.sort()\n",
        "FileContents=[]\n",
        "for path in alldirs:\n",
        "  #print(path)\n",
        "  file = open(path,'rb')\n",
        "  FileContents.append(file.read()) "
      ],
      "metadata": {
        "id": "DF13B20nq_JZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre processing"
      ],
      "metadata": {
        "id": "n_cnRZFMilMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens=[]\n",
        "for i in range(len(FileContents)):\n",
        "  #Conversion to lowercase\n",
        "  FileContents[i]=str(FileContents[i].lower()).replace(\"\\\\n\",\" \").replace(\"\\\\r\",\" \").replace(\"\\\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\t\",\" \").strip()\n",
        "  #Remove Special Characters\n",
        "  FileContents[i]=str(re.sub('[^A-Za-z0-9]+', ' ',FileContents[i]))\n",
        "  #Word Tokenization\n",
        "  nltk_tokens.append(nltk.word_tokenize(FileContents[i]))\n",
        "\n",
        "\n",
        "#Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_output=[]\n",
        "for i in range(len(nltk_tokens)):\n",
        "  lemmatized_output.append([lemmatizer.lemmatize(w) for w in nltk_tokens[i]])  \n",
        "  \n",
        "  \n",
        "#Remove Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "preprocessed=[]\n",
        "for i in lemmatized_output:\n",
        "\ttemp=[]\n",
        "\tfor r in i:\n",
        "\t\tif not r in stop_words:\n",
        "\t\t\ttemp.append(r)\n",
        "\tpreprocessed.append(temp)"
      ],
      "metadata": {
        "id": "abhMkqJMmu3u"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverted Index"
      ],
      "metadata": {
        "id": "yIZrzI_tdxPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict = {}\n",
        "  \n",
        "for i in range(len(preprocessed)):\n",
        "    tokens=preprocessed[i]\n",
        "    for item in tokens:\n",
        "        if item not in dict:\n",
        "            dict[item] = set()\n",
        "\n",
        "        if item in dict:\n",
        "            dict[item].add(i)"
      ],
      "metadata": {
        "id": "lxOSBIrfdy8L"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "J2UyU5LhgrJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the set of docs in which a token is present\n",
        "def finddocset(input):\n",
        "  if input in dict:\n",
        "    return dict[input]\n",
        "  else:\n",
        "    return set()"
      ],
      "metadata": {
        "id": "SThpT4lXgziy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OR(set1,set2):\n",
        "  list1=list(set1)\n",
        "  list2=list(set2)\n",
        "  list1.sort()\n",
        "  list2.sort()\n",
        "  num_comp=0\n",
        "  i=0\n",
        "  j=0\n",
        "  result=[]\n",
        "  while i<len(list1) and j<len(list2):\n",
        "    if list1[i]<list2[j]:\n",
        "      result.append(list1[i])\n",
        "      i+=1\n",
        "    elif list1[i]>list2[j]:\n",
        "      result.append(list2[j])\n",
        "      j+=1\n",
        "    else:\n",
        "      result.append(list1[i])\n",
        "      i+=1\n",
        "      result.append(list2[j])\n",
        "      j+=1\n",
        "    num_comp+=1\n",
        "\n",
        "  while i<len(list1):\n",
        "    result.append(list1[i])\n",
        "    i+=1\n",
        "  \n",
        "  while j<len(list2):\n",
        "    result.append(list2[j])\n",
        "    j+=1\n",
        "\n",
        "  result=set(result)\n",
        "  return (result,num_comp)\n"
      ],
      "metadata": {
        "id": "8-WARp4zYnAh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AND(set1,set2):\n",
        "  list1=list(set1)\n",
        "  list2=list(set2)\n",
        "  list1.sort()\n",
        "  list2.sort()\n",
        "  len1=len(list1)\n",
        "  len2=len(list2)\n",
        "\n",
        "  if len1>len2:\n",
        "    temp=list1\n",
        "    list1=list2\n",
        "    list2=temp\n",
        "\n",
        "  # Now list1 has less elements than lest2\n",
        "  d={}\n",
        "  for k in list2:\n",
        "    d[k]=1\n",
        "  \n",
        "  result=[]\n",
        "  i=0\n",
        "  while i<len(list1):\n",
        "    if list1[i] in d:\n",
        "      result.append(list1[i])\n",
        "    i+=1\n",
        "\n",
        "  num_comp=len(list1)\n",
        "  \n",
        "  result=set(result)\n",
        "  return (result,num_comp)\n"
      ],
      "metadata": {
        "id": "MR5iQszJaan_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANDNOT(set1,set2):\n",
        "  list1=list(set1)\n",
        "  list2=list(set2)\n",
        "  list1.sort()\n",
        "  list2.sort()\n",
        "  \n",
        "  d={}\n",
        "  for k in list2:\n",
        "    d[k]=1\n",
        "  \n",
        "  result=[]\n",
        "  i=0\n",
        "  while i<len(list1):\n",
        "    if list1[i] not in d:\n",
        "      result.append(list1[i])\n",
        "    i+=1\n",
        "\n",
        "  num_comp=len(list1)\n",
        "  \n",
        "  result=set(result)\n",
        "  return (result,num_comp)"
      ],
      "metadata": {
        "id": "XXnUu11gsnmn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NOT\n",
        "def NOT(set1):\n",
        "  universe=set()\n",
        "  for i in range(len(alldirs)):\n",
        "    universe.add(i)\n",
        "\n",
        "  return universe.difference(set1)"
      ],
      "metadata": {
        "id": "6SkCwyaimsUy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUERY PREPROCESSING\n",
        "\n",
        "ip='lion stood thoughtfully for a moment'\n",
        "#ip='telephone paved roads'\n",
        "operators=['OR','OR','OR']\n",
        "nltk_tokens=[]\n",
        "\n",
        "#Conversion to lowercase\n",
        "ip=str(ip.lower()).replace(\"\\\\n\",\" \").replace(\"\\\\r\",\" \").replace(\"\\\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\t\",\" \").strip()\n",
        "#Remove Special Characters\n",
        "ip=str(re.sub('[^A-Za-z0-9]+', ' ',ip))\n",
        "#Word Tokenization\n",
        "nltk_tokens.append(nltk.word_tokenize(ip))\n",
        "#Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_output=[]\n",
        "for i in range(len(nltk_tokens)):\n",
        "  lemmatized_output.append([lemmatizer.lemmatize(w) for w in nltk_tokens[i]]) \n",
        "\n",
        "  \n",
        "  \n",
        "#Remove Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens=[]\n",
        "for i in lemmatized_output:\n",
        "\ttemp=[]\n",
        "\tfor r in i:\n",
        "\t\tif not r in stop_words:\n",
        "\t\t\ttemp.append(r)\n",
        "\ttokens.append(temp)\n",
        " \n",
        "tokens=tokens[0]"
      ],
      "metadata": {
        "id": "96pI3_mMuTeU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaIvhHEqL_cg",
        "outputId": "b741fcc9-02bc-4afe-a222-94306617798d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lion', 'stood', 'thoughtfully', 'moment']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m=0\n",
        "if len(tokens)!=len(operators)+1:\n",
        "  print(\"Incorrect Number of operands\")\n",
        "else:\n",
        "  result=finddocset(tokens[0])\n",
        "  comp=0\n",
        "  # Handle the case that a token is not present in the document\n",
        "  while m<len(operators):\n",
        "    if operators[m]=='OR':\n",
        "      (result,c)=OR(result,finddocset(tokens[m+1]))\n",
        "      comp+=c\n",
        "    if operators[m]=='AND':\n",
        "      (result,c)=AND(result,finddocset(tokens[m+1]))\n",
        "      comp+=c\n",
        "    if operators[m]=='AND NOT':\n",
        "      (result,c)=ANDNOT(result,finddocset(tokens[m+1]))\n",
        "      comp+=c\n",
        "    if operators[m]=='OR NOT':\n",
        "      '''(result,c)=AND(result,finddocset(tokens[m+1]))\n",
        "      comp+=c'''\n",
        "      temp=NOT(finddocset(tokens[m+1]))\n",
        "      (result,c)=OR(result,temp)\n",
        "      comp+=c\n",
        "    m+=1\n",
        "  "
      ],
      "metadata": {
        "id": "Ug_7UMh_VKMs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(len(result))+\" documents were retrieved\")\n",
        "print(str(comp)+\" comparisons were made\")\n",
        "filenames=[]\n",
        "for i in result:\n",
        "  filenames.append(alldirs[i][31:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEcOY-lCwBuM",
        "outputId": "0b44f2b1-5fb9-4962-8041-542a08e51c1c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "211 documents were retrieved\n",
            "400 comparisons were made\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The list of documents are\")\n",
        "filenames.sort()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgcMJGdGzd8Q",
        "outputId": "d222c6f0-3777-4dac-fca0-a2826d254698"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The list of documents are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXij-Af5-5KS",
        "outputId": "69a8befe-e12d-4152-deb0-22b458fed91e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a_tv_t-p.com',\n",
              " 'aeonint.txt',\n",
              " 'allusion',\n",
              " 'ambrose.bie',\n",
              " 'anim_lif.txt',\n",
              " 'anime.lif',\n",
              " 'annoy.fascist',\n",
              " 'art-fart.hum',\n",
              " 'b-2.jok',\n",
              " 'badday.hum',\n",
              " 'barney.txt',\n",
              " 'bbh_intv.txt',\n",
              " 'beauty.tm',\n",
              " 'beesherb.txt',\n",
              " 'bitnet.txt',\n",
              " 'bmdn01.txt',\n",
              " 'boneles2.txt',\n",
              " 'butwrong.hum',\n",
              " 'bw-phwan.hat',\n",
              " 'bw.txt',\n",
              " 'cabbage.txt',\n",
              " 'calculus.txt',\n",
              " 'candy.txt',\n",
              " 'cartoon.law',\n",
              " 'cartoon.laws',\n",
              " 'cartoon_.txt',\n",
              " 'chickenheadbbs.txt',\n",
              " 'childhoo.jok',\n",
              " 'christop.int',\n",
              " 'clancy.txt',\n",
              " 'classicm.hum',\n",
              " 'cmu.share',\n",
              " 'cogdis.txt',\n",
              " 'collected_quotes.txt',\n",
              " 'commutin.jok',\n",
              " 'computer.txt',\n",
              " 'conan.txt',\n",
              " 'consp.txt',\n",
              " 'cookie.1',\n",
              " 'coyote.txt',\n",
              " 'cuchy.hum',\n",
              " 'cybrtrsh.txt',\n",
              " 'dead3.txt',\n",
              " 'dead4.txt',\n",
              " 'dead5.txt',\n",
              " 'deep.txt',\n",
              " 'devils.jok',\n",
              " 'dingding.hum',\n",
              " 'doggun.sto',\n",
              " 'drinks.gui',\n",
              " 'dromes.txt',\n",
              " 'drunk.txt',\n",
              " 'dthought.txt',\n",
              " 'econridl.fun',\n",
              " 'engineer.hum',\n",
              " 'epi_.txt',\n",
              " 'epi_tton.txt',\n",
              " 'episimp2.txt',\n",
              " 'epitaph',\n",
              " 'eskimo.nel',\n",
              " 'exam.50',\n",
              " 'facedeth.txt',\n",
              " 'fascist.txt',\n",
              " 'female.jok',\n",
              " 'filmgoof.txt',\n",
              " 'flux_fix.txt',\n",
              " 'fuckyou2.txt',\n",
              " 'gas.txt',\n",
              " 'gd_gal.txt',\n",
              " 'gd_ql.txt',\n",
              " 'ghostfun.hum',\n",
              " 'golnar.txt',\n",
              " 'gown.txt',\n",
              " 'grail.txt',\n",
              " 'hackingcracking.txt',\n",
              " 'hackmorality.txt',\n",
              " 'homebrew.txt',\n",
              " 'humor9.txt',\n",
              " 'idr2.txt',\n",
              " 'incarhel.hum',\n",
              " 'indgrdn.txt',\n",
              " 'initials.rid',\n",
              " 'insult.lst',\n",
              " 'insults1.txt',\n",
              " 'is_story.txt',\n",
              " 'ivan.hum',\n",
              " 'japantv.txt',\n",
              " 'jayjay.txt',\n",
              " 'jc-elvis.inf',\n",
              " 'kaboom.hum',\n",
              " 'kanalx.txt',\n",
              " 'lawyer.jok',\n",
              " 'lbinter.hum',\n",
              " 'let.go',\n",
              " 'letgosh.txt',\n",
              " 'lif&love.hum',\n",
              " 'lifeimag.hum',\n",
              " 'lifeonledge.txt',\n",
              " 'lion.jok',\n",
              " 'lion.txt',\n",
              " 'lions.cat',\n",
              " 'llong.hum',\n",
              " 'lozerzon.hum',\n",
              " 'luggage.hum',\n",
              " 'luvstory.txt',\n",
              " 'm0dzmen.hum',\n",
              " 'maecenas.hum',\n",
              " 'mailfrag.hum',\n",
              " 'manners.txt',\n",
              " 'marriage.hum',\n",
              " 'mash.hum',\n",
              " 'mcd.txt',\n",
              " 'meinkamp.hum',\n",
              " 'mel.txt',\n",
              " 'mindvox',\n",
              " 'minn.txt',\n",
              " 'misc.1',\n",
              " 'mlverb.hum',\n",
              " 'montpyth.hum',\n",
              " 'moore.txt',\n",
              " 'moose.txt',\n",
              " 'msorrow',\n",
              " 'mundane.v2',\n",
              " 'murphy_l.txt',\n",
              " 'murphys.txt',\n",
              " 'myheart.hum',\n",
              " 'namaste.txt',\n",
              " 'nameisreo.txt',\n",
              " 'news.hum',\n",
              " 'nigel.10',\n",
              " 'nigel.3',\n",
              " 'nigel.5',\n",
              " 'nigel10.txt',\n",
              " 'nihgel_8.9',\n",
              " 'nukewar.txt',\n",
              " 'oldeng.hum',\n",
              " 'oliver.txt',\n",
              " 'oliver02.txt',\n",
              " 'onetoone.hum',\n",
              " 'onetotwo.hum',\n",
              " 'oxymoron.jok',\n",
              " 'passage.hum',\n",
              " 'passenge.sim',\n",
              " 'peatchp.hum',\n",
              " 'pepper.txt',\n",
              " 'pepsideg.txt',\n",
              " 'petshop',\n",
              " 'phorse.hum',\n",
              " 'pizzawho.hum',\n",
              " 'policpig.hum',\n",
              " 'popmusi.hum',\n",
              " 'prac1.jok',\n",
              " 'prac2.jok',\n",
              " 'prac3.jok',\n",
              " 'prac4.jok',\n",
              " 'pracjoke.txt',\n",
              " 'practica.txt',\n",
              " 'pro-fact.hum',\n",
              " 'progrs.gph',\n",
              " 'psycho.txt',\n",
              " 'pukeprom.jok',\n",
              " 'puzzles.jok',\n",
              " 'quack26.txt',\n",
              " 'quest.hum',\n",
              " 'quotes.txt',\n",
              " 'quux_p.oem',\n",
              " 'radiolaf.hum',\n",
              " 'reasons.txt',\n",
              " 'reeves.txt',\n",
              " 'rns_ency.txt',\n",
              " 'scratchy.txt',\n",
              " 'sfmovie.txt',\n",
              " 'shuttleb.hum',\n",
              " 'smurfkil.hum',\n",
              " 'snapple.rum',\n",
              " 'socecon.hum',\n",
              " 'solders.hum',\n",
              " 'soleleer.hum',\n",
              " 'stone.hum',\n",
              " 'strine.txt',\n",
              " 'stuf10.txt',\n",
              " 'stuf11.txt',\n",
              " 'suicide2.txt',\n",
              " 'sw_err.txt',\n",
              " \"terrmcd'.hum\",\n",
              " 'tfepisod.hum',\n",
              " 'three.txt',\n",
              " 'throwawa.hum',\n",
              " 'timetr.hum',\n",
              " 'tnd.1',\n",
              " 'top10.txt',\n",
              " 'top10st2.txt',\n",
              " 'tpquotes.txt',\n",
              " 'ukunderg.txt',\n",
              " 'valujet.txt',\n",
              " 'various.txt',\n",
              " 'vonthomp',\n",
              " 'wacky.ani',\n",
              " 'wagon.hum',\n",
              " 'wedding.hum',\n",
              " 'whoops.hum',\n",
              " 'wimptest.txt',\n",
              " 'worldend.hum',\n",
              " 'xibovac.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del input"
      ],
      "metadata": {
        "id": "E_Sp_QEbFEP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_queries=input(\"Enter Number of Queries \")\n",
        "number_of_queries=int(number_of_queries)\n",
        "\n",
        "while number_of_queries>0:\n",
        "  try:\n",
        "    del ip\n",
        "    del op\n",
        "  except:\n",
        "    pass\n",
        "  number_of_queries-=1\n",
        "  ip=input(\"Enter Query sentence \")\n",
        "  op=input(\"Enter Operands \")\n",
        "  op=op.split(',')\n",
        "  op[0]=op[0][1:]\n",
        "  op[-1]=op[-1][:-1]\n",
        "  print(op)\n",
        "  nltk_tokens=[]\n",
        "\n",
        "  #Conversion to lowercase\n",
        "  ip=str(ip.lower()).replace(\"\\\\n\",\" \").replace(\"\\\\r\",\" \").replace(\"\\\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\t\",\" \").strip()\n",
        "  #Remove Special Characters\n",
        "  ip=str(re.sub('[^A-Za-z0-9]+', ' ',ip))\n",
        "  #Word Tokenization\n",
        "  nltk_tokens.append(nltk.word_tokenize(ip))\n",
        "  #Lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_output=[]\n",
        "  for i in range(len(nltk_tokens)):\n",
        "    lemmatized_output.append([lemmatizer.lemmatize(w) for w in nltk_tokens[i]]) \n",
        "\n",
        "    \n",
        "    \n",
        "  #Remove Stop Words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens=[]\n",
        "  for i in lemmatized_output:\n",
        "    temp=[]\n",
        "    for r in i:\n",
        "      if not r in stop_words:\n",
        "        temp.append(r)\n",
        "    tokens.append(temp)\n",
        "  \n",
        "  tokens=tokens[0]\n",
        "  m=0\n",
        "  if len(tokens)!=len(operators)+1:\n",
        "    print(\"Incorrect Number of operands\")\n",
        "    continue\n",
        "  else:\n",
        "    result=finddocset(tokens[0])\n",
        "    comp=0\n",
        "    # Handle the case that a token is not present in the document\n",
        "    while m<len(operators):\n",
        "      if operators[m]=='OR':\n",
        "        (result,c)=OR(result,finddocset(tokens[m+1]))\n",
        "        comp+=c\n",
        "      if operators[m]=='AND':\n",
        "        (result,c)=AND(result,finddocset(tokens[m+1]))\n",
        "        comp+=c\n",
        "      if operators[m]=='AND NOT':\n",
        "        (result,c)=ANDNOT(result,finddocset(tokens[m+1]))\n",
        "        comp+=c\n",
        "      if operators[m]=='OR NOT':\n",
        "        \n",
        "        temp=NOT(finddocset(tokens[m+1]))\n",
        "        (result,c)=OR(result,temp)\n",
        "        comp+=c\n",
        "      m+=1\n",
        "\n",
        "  print(str(len(result))+\" documents were retrieved\")\n",
        "  print(str(comp)+\" comparisons were made\")\n",
        "  filenames=[]\n",
        "  for i in result:\n",
        "    filenames.append(alldirs[i][31:])\n",
        "\n",
        "  print(\"The list of documents are\")\n",
        "  print(filenames)\n",
        "  del ip\n",
        "  del op\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ddz_8l4Ea02",
        "outputId": "7aac520f-5e1f-461b-d3f0-176069b0193d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Number of Queries 1\n",
            "Enter Query sentence lion stood thoughtfully for a moment\n",
            "Enter Operands [OR,OR,OR]\n",
            "['OR', 'OR', 'OR']\n",
            "208 documents were retrieved\n",
            "391 comparisons were made\n",
            "The list of documents are\n",
            "['initials.rid', 'a_tv_t-p.com', 'three.txt', 'throwawa.hum', 'insult.lst', 'insults1.txt', 'timetr.hum', 'tnd.1', 'top10.txt', 'top10st2.txt', 'is_story.txt', 'tpquotes.txt', 'ivan.hum', 'aeonint.txt', 'japantv.txt', 'jayjay.txt', 'jc-elvis.inf', 'allusion', 'ambrose.bie', 'ukunderg.txt', 'anim_lif.txt', 'anime.lif', 'annoy.fascist', 'valujet.txt', 'kaboom.hum', 'kanalx.txt', 'various.txt', 'vonthomp', 'wacky.ani', 'wagon.hum', 'art-fart.hum', 'wedding.hum', 'lawyer.jok', 'b-2.jok', 'lbinter.hum', 'whoops.hum', 'let.go', 'letgosh.txt', 'badday.hum', 'wimptest.txt', 'lif&love.hum', 'lifeimag.hum', 'lifeonledge.txt', 'barney.txt', 'lion.jok', 'lion.txt', 'lions.cat', 'worldend.hum', 'bbh_intv.txt', 'llong.hum', 'beauty.tm', 'xibovac.txt', 'lozerzon.hum', 'luggage.hum', 'beesherb.txt', 'luvstory.txt', 'm0dzmen.hum', 'maecenas.hum', 'mailfrag.hum', 'bitnet.txt', 'manners.txt', 'marriage.hum', 'mash.hum', 'bmdn01.txt', 'mcd.txt', 'meinkamp.hum', 'mel.txt', 'boneles2.txt', 'booze1.fun', 'mindvox', 'minn.txt', 'misc.1', 'mlverb.hum', 'montpyth.hum', 'moore.txt', 'moose.txt', 'butwrong.hum', 'msorrow', 'bw-phwan.hat', 'bw.txt', 'mundane.v2', 'cabbage.txt', 'caesardr.sal', 'murphy_l.txt', 'murphys.txt', 'calculus.txt', 'myheart.hum', 'namaste.txt', 'nameisreo.txt', 'candy.txt', 'cartoon.law', 'cartoon.laws', 'cartoon_.txt', 'news.hum', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.5', 'nigel10.txt', 'nihgel_8.9', 'chickenheadbbs.txt', 'childhoo.jok', 'nukewar.txt', 'christop.int', 'clancy.txt', 'classicm.hum', 'cmu.share', 'cogdis.txt', 'oldeng.hum', 'oliver.txt', 'oliver02.txt', 'onetoone.hum', 'collected_quotes.txt', 'onetotwo.hum', 'commutin.jok', 'computer.txt', 'conan.txt', 'oxymoron.jok', 'consp.txt', 'cookie.1', 'passage.hum', 'passenge.sim', 'coyote.txt', 'peatchp.hum', 'cuchy.hum', 'pepper.txt', 'pepsideg.txt', 'petshop', 'phorse.hum', 'cybrtrsh.txt', 'pizzawho.hum', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'policpig.hum', 'deep.txt', 'popmusi.hum', 'devils.jok', 'dingding.hum', 'prac1.jok', 'prac2.jok', 'prac3.jok', 'prac4.jok', 'pracjoke.txt', 'practica.txt', 'doggun.sto', 'pro-fact.hum', 'progrs.gph', 'drinks.gui', 'dromes.txt', 'drunk.txt', 'dthought.txt', 'psycho.txt', 'pukeprom.jok', 'econridl.fun', 'puzzles.jok', 'quack26.txt', 'engineer.hum', 'quest.hum', 'quotes.txt', 'quux_p.oem', 'epi_.txt', 'radiolaf.hum', 'epi_tton.txt', 'episimp2.txt', 'epitaph', 'eskimo.nel', 'exam.50', 'reasons.txt', 'facedeth.txt', 'fascist.txt', 'female.jok', 'reeves.txt', 'filmgoof.txt', 'flux_fix.txt', 'rns_ency.txt', 'fuckyou2.txt', 'scratchy.txt', 'gas.txt', 'sfmovie.txt', 'gd_gal.txt', 'shuttleb.hum', 'gd_ql.txt', 'ghostfun.hum', 'smurfkil.hum', 'golnar.txt', 'snapple.rum', 'gown.txt', 'grail.txt', 'socecon.hum', 'solders.hum', 'soleleer.hum', 'hackingcracking.txt', 'hackmorality.txt', 'hecomes.jok', 'stone.hum', 'strine.txt', 'stuf10.txt', 'stuf11.txt', 'suicide2.txt', 'sw_err.txt', 'homebrew.txt', \"terrmcd'.hum\", 'humor9.txt', 'idr2.txt', 'tfepisod.hum', 'incarhel.hum', 'indgrdn.txt']\n"
          ]
        }
      ]
    }
  ]
}