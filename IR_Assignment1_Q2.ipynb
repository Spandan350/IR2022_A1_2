{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment1_Q2",
      "provenance": [],
      "collapsed_sections": [
        "P-VKparYiY55",
        "3Yv1fX9xSZwr",
        "CfUb96ZJSlFw",
        "sLd33TnQe2TP",
        "8TvTbvOrMySm",
        "zIP7e8g-nSMj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzksUXH8dFuR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "import re\n",
        "import io\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c24s0nmdOBL",
        "outputId": "f5624f99-d87e-4383-c75a-cf19d45d7ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mU02-AwdSzp",
        "outputId": "a0afde63-9d16-445d-b1be-638d0f630905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/MyDrive/MyDrive/Humor,Hist,Media,Food.zip > /dev/null"
      ],
      "metadata": {
        "id": "JJ9Ht0qXdWpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path='/content/Humor,Hist,Media,Food'\n",
        "alldirs = [dataset_path+'/'+d for d in os.listdir(os.path.join(dataset_path))]"
      ],
      "metadata": {
        "id": "1l-jGGd-eFR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alldirs.sort()"
      ],
      "metadata": {
        "id": "zqMto0wYRR9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path='/content/Humor,Hist,Media,Food'\n",
        "alldirs = [dataset_path+'/'+d for d in os.listdir(os.path.join(dataset_path))]\n",
        "FileContents=[]\n",
        "\n",
        "for path in alldirs:\n",
        "  file = open(path,'rb')\n",
        "  FileContents.append(file.read())"
      ],
      "metadata": {
        "id": "07cm2Xi2fMO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-2 (a) Preprocessing"
      ],
      "metadata": {
        "id": "uJKLB_Z1g6lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting text to lowercase"
      ],
      "metadata": {
        "id": "8DZji69LhA0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(FileContents)):\n",
        "  FileContents[i]=str(FileContents[i].lower()).replace(\"\\\\n\",\"\").replace(\"\\\\r\",\"\").replace(\"\\\\t\",\"\").strip()\n"
      ],
      "metadata": {
        "id": "Laz00tMEg8nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Special Characters"
      ],
      "metadata": {
        "id": "P-VKparYiY55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(FileContents)):\n",
        "  FileContents[i]=str(re.sub('[^A-Za-z0-9]+', ' ',FileContents[i]))"
      ],
      "metadata": {
        "id": "nFPa5qB0hJpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Tokenization"
      ],
      "metadata": {
        "id": "3Yv1fX9xSZwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens=[]\n",
        "\n",
        "for i in range(len(FileContents)):\n",
        "  nltk_tokens.append(nltk.word_tokenize(FileContents[i]))"
      ],
      "metadata": {
        "id": "jrB-AgVCScaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove Stop Words"
      ],
      "metadata": {
        "id": "CfUb96ZJSlFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Use this to read file content as a stream:\n",
        "preprocessed=[]\n",
        "for i in nltk_tokens:\n",
        "\ttemp=[]\n",
        "\tfor r in i:\n",
        "\t\tif not r in stop_words:\n",
        "\t\t\ttemp.append(r)\n",
        "\tpreprocessed.append(temp)\n"
      ],
      "metadata": {
        "id": "mZ1gxIGXSnzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-2 (b) Creating Positional Index Data Structure "
      ],
      "metadata": {
        "id": "sLd33TnQe2TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#positional Index data Structure \n",
        "positionalIndex={}\n",
        "\n",
        "for doc_id in range(len(preprocessed)):\n",
        "  document = preprocessed[doc_id]\n",
        "  for index in range(len(document)):\n",
        "    word = document[index]\n",
        "\n",
        "    if(word in positionalIndex):\n",
        "      doc_dict = positionalIndex[word]\n",
        "      if(doc_id in doc_dict):\n",
        "        doc_dict[doc_id].append(index)\n",
        "      else:\n",
        "        doc_dict[doc_id]=[index]\n",
        "      positionalIndex[word]=doc_dict\n",
        "\n",
        "    else:\n",
        "      doc_dict = {}\n",
        "      doc_dict[doc_id]=[index]\n",
        "      positionalIndex[word]=doc_dict\n"
      ],
      "metadata": {
        "id": "GT12kdhafwl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q-2 (c) Taking input the phrase queries"
      ],
      "metadata": {
        "id": "JxvX-C2_wTIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the query"
      ],
      "metadata": {
        "id": "8TvTbvOrMySm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessingQuery(query):\n",
        "  query = query.lower().replace(\"\\\\n\",\"\").replace(\"\\\\r\",\"\").replace(\"\\\\t\",\"\").strip()\n",
        "  \n",
        "  # word tokenization\n",
        "  temp_words = nltk.word_tokenize(query)\n",
        "\n",
        "  # removing the stop words\n",
        "  words=[]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for word in temp_words:\n",
        "\t  if word not in stop_words:\n",
        "\t\t  words.append(word)\n",
        "  \n",
        "  return words"
      ],
      "metadata": {
        "id": "my-_JcpcMw-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrievingDocuments(query_index,query_words,document_id,word_index,positionalIndex,retrieved_doc_ids):\n",
        "  if(query_index==len(query_words)):\n",
        "    retrieved_doc_ids.add(document_id)\n",
        "    return\n",
        "  \n",
        "  if(query_index==0):\n",
        "    # retrieving all the documents that contains the word query_words[query_index]\n",
        "    doc_dict = positionalIndex[query_words[query_index]]\n",
        "\n",
        "    if(doc_dict is not None):\n",
        "      # retrieving individual documents containing the word query_words[query_index]\n",
        "\n",
        "      for doc_id in doc_dict.keys():\n",
        "        # retrieving the position of the word query_words[query_index] inside the document doc_id\n",
        "\n",
        "        index_list = doc_dict[doc_id]\n",
        "        for index in index_list:\n",
        "          # taking each index as our starting point of the phrase query\n",
        "\n",
        "          retrievingDocuments(query_index+1,query_words,doc_id,index+1,positionalIndex,retrieved_doc_ids)\n",
        "  else:\n",
        "    doc_dict = positionalIndex[query_words[query_index]]\n",
        "\n",
        "    if(document_id in doc_dict):\n",
        "      index_list = doc_dict[document_id]\n",
        "      \n",
        "      if(word_index in index_list):\n",
        "        retrievingDocuments(query_index+1,query_words,document_id,word_index+1,positionalIndex,retrieved_doc_ids)\n"
      ],
      "metadata": {
        "id": "AudhdbvhQ_L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking input phrase queries"
      ],
      "metadata": {
        "id": "zIP7e8g-nSMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_queries = ['turbo encabulator', 'usual thing', 'further salute']\n",
        "\n",
        "for query in phrase_queries:\n",
        "  query_words=preprocessingQuery(query)\n",
        "  \n",
        "  pos=0\n",
        "  retrieved_doc_ids = set()\n",
        "  retrievingDocuments(0,query_words,-1,-1,positionalIndex,retrieved_doc_ids)\n",
        "\n",
        "  retrieved_doc_names=[]\n",
        "  for doc_id in sorted(retrieved_doc_ids):\n",
        "    retrieved_doc_names.append(alldirs[doc_id].split('/')[-1])\n",
        "\n",
        "  print(\"Actual Phrase Query: \",query)\n",
        "  print(\"Preprocessed Phrase Query words: \",query_words)\n",
        "  print(\"Number of documents retrieved: \",len(retrieved_doc_names))\n",
        "  print(\"List of the document names retrieved: \\n\",retrieved_doc_names)\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "_B164hJnvEDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c022e67-53f4-4823-e06c-a83bcb4f752f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Phrase Query:  turbo encabulator\n",
            "Preprocessed Phrase Query words:  ['turbo', 'encabulator']\n",
            "Number of documents retrieved:  1\n",
            "List of the document names retrieved: \n",
            " ['turbo.hum']\n",
            "\n",
            "\n",
            "Actual Phrase Query:  usual thing\n",
            "Preprocessed Phrase Query words:  ['usual', 'thing']\n",
            "Number of documents retrieved:  2\n",
            "List of the document names retrieved: \n",
            " ['critic.txt', 'banana01.brd']\n",
            "\n",
            "\n",
            "Actual Phrase Query:  further salute\n",
            "Preprocessed Phrase Query words:  ['salute']\n",
            "Number of documents retrieved:  10\n",
            "List of the document names retrieved: \n",
            " ['turbo.hum', 'fwksfun.hum', 'prover.wisom', 'arnold.txt', 'fireplacein.txt', 'mlverb.hum', 'reconcil.hum', 'idaho.txt', 'prover_w.iso', 'oliver02.txt']\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}